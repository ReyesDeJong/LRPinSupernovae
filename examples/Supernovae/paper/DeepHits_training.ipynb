{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Deep-HiTS\n",
    "\n",
    "### Deep-HiTS model from paper, trainning under LRP framework to save model's parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 2 and 3 comptibility\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "#enables acces to parent folder ()\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "#other imports\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#modules import\n",
    "import modules.sequential32 as sequential32\n",
    "import modules.linear32 as linear32\n",
    "import modules.convolution32 as convolution32\n",
    "import modules.maxpool32 as maxpool32\n",
    "import modules.rotation as rotation\n",
    "\n",
    "\n",
    "#print of plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of samples for every trainning iteration\n",
    "BATCH_SIZE = 50\n",
    "#First 100k iterations of trainning\n",
    "INITIAL_PATIENCE = 100000  \n",
    "#Validate trainning every 10k iterations\n",
    "VALIDATION_PERIOD = 10000  \n",
    "#Update learning rate every 100k iterations\n",
    "ANNEALING_PERIOD = 100000  \n",
    "#If validation criteria is met, increase trainning iterations by 100k\n",
    "PATIENCE_INCREMENT = 100000\n",
    "#path where summaries, graph and best  trained models will be saved\n",
    "SUMMARY_DIR = \"TB/deep_hits_runs/\"\n",
    "#path to *.tfrecord files (data) in your computer\n",
    "whole_data_path = '/home/tesla/Desktop/LRP/deep_hits_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to acces data base\n",
    "\n",
    "To do this, you must have HiTS 2013 database in tfrecords format; train (1.2 million samples), validation (100k samples) and test (100k samples) set. You can finde them on __TODO__...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to get certain dataset tensor\n",
    "def get_dataset_tensor(dataset):\n",
    "    data_path = whole_data_path+dataset\n",
    "    feature = {'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "               'label': tf.FixedLenFeature([], tf.int64),\n",
    "               'snr': tf.FixedLenFeature([], tf.string)}\n",
    "    # Create a list of filenames and pass it to a queue\n",
    "    filename_queue = tf.train.string_input_producer([data_path])\n",
    "    # Define a reader and read the next record\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    # Decode the record read by the reader\n",
    "    features = tf.parse_single_example(serialized_example, features=feature)\n",
    "    # Convert the image data from string back to the numbers\n",
    "    image = tf.decode_raw(features['image_raw'], tf.float32)\n",
    "\n",
    "    # Cast label data into int32\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    \n",
    "    # Cast snr data into int32\n",
    "    snr = tf.decode_raw(features['snr'], tf.float64)\n",
    "    \n",
    "    # Reshape image data into the original shape\n",
    "    image = tf.reshape(image, [21, 21, 4])\n",
    "    \n",
    "    snr = tf.reshape(snr, [1])\n",
    "\n",
    "    # Any preprocessing here ...\n",
    "\n",
    "    # Creates batches by randomly shuffling tensors\n",
    "    images, labels, snrs= tf.train.batch([image, label, snr],\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    capacity=100000,\n",
    "                                    num_threads=1)\n",
    "    return images, labels, snrs\n",
    "\n",
    "def get_train_tensors():\n",
    "    return get_dataset_tensor('snr_train.tfrecord')\n",
    "\n",
    "\n",
    "def get_validation_tensors():\n",
    "    return get_dataset_tensor('snr_validation.tfrecord')\n",
    "\n",
    "\n",
    "def get_test_tensors():\n",
    "    return get_dataset_tensor('snr_test.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to perform rotation\n",
    "Define function for rotate tensor in [0,90,180,270] degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "given an image batch tensor as [batch_size,height,width,channels], \n",
    "generate 4 rotated versions in [0,90,180,270] degrees and concatenate them in batch dimension \n",
    "\n",
    "@param img_batch input tensor thath contains images\n",
    "@return a [4*batch_size,height,width,channels] version of img_batch\n",
    "\"\"\"\n",
    "def augment_with_rotations(img_batch):   \n",
    "    #perform rotations\n",
    "    images90 = tf.map_fn(lambda x: tf.image.rot90(x, k=1), img_batch)\n",
    "    images180 = tf.map_fn(lambda x: tf.image.rot90(x, k=2), img_batch)\n",
    "    images270 = tf.map_fn(lambda x: tf.image.rot90(x, k=3), img_batch)\n",
    "    \n",
    "    #concatenate along first tensor dimension (batch dimension)\n",
    "    return tf.concat([img_batch,\n",
    "                         images90,\n",
    "                         images180,\n",
    "                         images270], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define network model\n",
    "\n",
    "Deep-HiTS model for LRP framework, exact same as in paper. It considers:\n",
    "\n",
    "1. Leaky-ReLU\n",
    "2. He weights initialization\n",
    "3. Biases initializated in 0.0\n",
    "4. Stretching layer after convolutions that concatenates features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn():\n",
    "    return sequential32.Sequential([\n",
    "        \n",
    "        convolution32.Convolution(kernel_size=4, output_depth=32,            \n",
    "                                  input_depth=4, input_dim=27, act ='lrelu',    \n",
    "                                  stride_size=1, pad='VALID',                         \n",
    "                                  weights_init= tf.truncated_normal_initializer(stddev=np.sqrt(2/(27*27*4))),\n",
    "                                  bias_init= tf.constant_initializer(0.0)),\n",
    "                                \n",
    "                                    \n",
    "        convolution32.Convolution(kernel_size=3, output_depth=32, stride_size=1, act ='lrelu', pad='SAME',\n",
    "                                  weights_init= tf.truncated_normal_initializer(stddev=np.sqrt(2/(24*24*32))),\n",
    "                                  bias_init= tf.constant_initializer(0.0)),\n",
    "\n",
    "                       \n",
    "        maxpool32.MaxPool(),\n",
    "\n",
    "        convolution32.Convolution(kernel_size=3, output_depth=64, stride_size=1, act ='lrelu', pad='SAME',\n",
    "                                  weights_init= tf.truncated_normal_initializer(stddev=np.sqrt(2/(12*12*32))),\n",
    "                                  bias_init= tf.constant_initializer(0.0)),\n",
    "\n",
    "                                    \n",
    "        convolution32.Convolution(kernel_size=3, output_depth=64, stride_size=1, act ='lrelu', pad='SAME',\n",
    "                                  weights_init= tf.truncated_normal_initializer(stddev=np.sqrt(2/(12*12*64))),\n",
    "                                  bias_init= tf.constant_initializer(0.0)),\n",
    "\n",
    "                                    \n",
    "        convolution32.Convolution(kernel_size=3, output_depth=64, stride_size=1, act ='lrelu', pad='SAME',\n",
    "                                  weights_init= tf.truncated_normal_initializer(stddev=np.sqrt(2/(12*12*64))),\n",
    "                                  bias_init= tf.constant_initializer(0.0)),\n",
    "\n",
    "        maxpool32.MaxPool(),\n",
    "                                    \n",
    "        rotation.Rotation(rotation_num=4),\n",
    "                       \n",
    "        linear32.Linear(64, act ='lrelu', keep_prob=keep_prob, use_dropout = True,\n",
    "                        weights_init= tf.truncated_normal_initializer(stddev=np.sqrt(2/(9216))),\n",
    "                        bias_init= tf.constant_initializer(0.0)),\n",
    "                                               \n",
    "        linear32.Linear(64, act ='lrelu', keep_prob=keep_prob, use_dropout = True,\n",
    "                        weights_init= tf.truncated_normal_initializer(stddev=np.sqrt(2/(64))),\n",
    "                        bias_init= tf.constant_initializer(0.0)),\n",
    "\n",
    "        linear32.Linear(2, act ='linear',\n",
    "                        weights_init= tf.truncated_normal_initializer(stddev=np.sqrt(2/(64))),\n",
    "                        bias_init= tf.constant_initializer(0.0))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass ... \n",
      "------------------------------------------------- \n",
      "input:: [None, 27, 27, 4]\n",
      "conv2d_1:: [None, 24, 24, 32]\n",
      "conv2d_2:: [None, 24, 24, 32]\n",
      "maxpool_3:: [None, 12, 12, 32]\n",
      "conv2d_4:: [None, 12, 12, 64]\n",
      "conv2d_5:: [None, 12, 12, 64]\n",
      "conv2d_6:: [None, 12, 12, 64]\n",
      "maxpool_7:: [None, 6, 6, 64]\n",
      "rotation_8:: [None, 9216]\n",
      "linear_9:: [None, 64]\n",
      "linear_10:: [None, 64]\n",
      "linear_11:: [None, 2]\n",
      "softmax:: [None, 2]\n",
      "\n",
      "------------------------------------------------- \n"
     ]
    }
   ],
   "source": [
    "#Dropout placeholder\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep-prob')\n",
    "\n",
    "#Input place holders and variables\n",
    "with tf.device('/cpu:0'):\n",
    "    #train tensors variables, run them to get dataset sample\n",
    "    images, labels, snr = get_train_tensors()\n",
    "    #validation tensors variables\n",
    "    validation_images, validation_labels, validation_snr = get_validation_tensors()\n",
    "    #test tensors variables\n",
    "    test_images, test_labels, test_snr = get_test_tensors()\n",
    "    \n",
    "    #place holder with default for input image, it enables to pass an specific tensor,\n",
    "    #or call 'images' to get a train tensor\n",
    "    images = tf.placeholder_with_default(images,\n",
    "                                         (None, 21, 21, 4),\n",
    "                                         'images_placeholder')\n",
    "    #place holder with default for input label, it enables to pass an specific label,\n",
    "    #or call 'labels' to get a train label\n",
    "    labels = tf.placeholder_with_default(labels,\n",
    "                                         None,\n",
    "                                         'labels_placeholder')\n",
    "    #label to one-hot\n",
    "    one_hot_labels = tf.one_hot(labels, 2, dtype=tf.float32)\n",
    "    #generate rotated images\n",
    "    augmented_input = augment_with_rotations(images)\n",
    "    #zero-pad images from 21x21 stamps to 27x27, to fit a 4x4 filter\n",
    "    padded_input = tf.pad(augmented_input,\n",
    "                          paddings=[\n",
    "                              [0, 0],\n",
    "                              [3, 3],\n",
    "                              [3, 3],\n",
    "                              [0, 0]])\n",
    "    \n",
    "    #input to model\n",
    "    inp=padded_input\n",
    "    #labels\n",
    "    y_=one_hot_labels\n",
    "\n",
    "#Model instance\n",
    "with tf.variable_scope('model'):\n",
    "        #instanciate model\n",
    "        net = nn()       \n",
    "        #feed-forward method and get score output. This iterates through layer in net object of class Sequiential\n",
    "        score = net.forward(inp)\n",
    "        #pass scores through softmax for network output ([0,1] probability)   \n",
    "        y = tf.nn.softmax(score)\n",
    "        #predicted classes\n",
    "        y_pred_cls = tf.argmax(y, 1)\n",
    "        #true classes\n",
    "        y_true_cls = tf.argmax(y_, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainning Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainning variables\n",
    "#Enable learning rate actualization through trainning\n",
    "with tf.variable_scope('train'):\n",
    "        #loss function\n",
    "        diff = tf.nn.softmax_cross_entropy_with_logits(logits=score, labels=y_)\n",
    "        #cost function\n",
    "        cost = tf.reduce_mean(diff)\n",
    "        #learning rate that will be actuallized every ANNEALING_PERIOD iterations\n",
    "        learning_rate = tf.Variable(0.04, trainable=False, collections=[])\n",
    "        #SDG optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        \n",
    "        #train operation to be run for performing a leraning iteration\n",
    "        train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement \n",
    "Build accuracy and cost list for summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):  \n",
    "        #compare predictions\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "        #get accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            \n",
    "#tensorflow summary        \n",
    "accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "#metrics to be used on validation criteria\n",
    "metrics = (cost, accuracy)\n",
    "#names for those metrics\n",
    "metrics_names = ('xentropy', 'accuracy')\n",
    "#list of metrics as a TF summary\n",
    "metric_summary_list = []\n",
    "\n",
    "#add metrics to summary list\n",
    "for metric, name in zip(metrics, metrics_names):\n",
    "    summary = tf.summary.scalar(name, metric)\n",
    "    metric_summary_list.append(summary)\n",
    "\n",
    "#get unique summary for validation criteria metrics\n",
    "merged_summaries = tf.summary.merge(metric_summary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other performance measures\n",
    "Explicit metrics to build Table II of paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance_measures'):\n",
    "    #confusion matrix values\n",
    "    with tf.name_scope('values'):\n",
    "        #true positives\n",
    "        TP = tf.count_nonzero((y_pred_cls * y_true_cls))\n",
    "        #true negative\n",
    "        TN = tf.count_nonzero((y_pred_cls - 1) * (y_true_cls- 1))\n",
    "        #false positives\n",
    "        FP = tf.count_nonzero(y_pred_cls * (y_true_cls - 1))\n",
    "        #false negatives\n",
    "        FN = tf.count_nonzero((y_pred_cls - 1) * y_true_cls)\n",
    "        \n",
    "    with tf.name_scope('accuracy_func'):\n",
    "            acc_mes = (TP+TN)/(TP+TN+FN+FP)\n",
    "        \n",
    "    with tf.name_scope('precision_func'):\n",
    "            prec_mes = TP/(TP+FP)\n",
    "            \n",
    "    with tf.name_scope('recall_func'):\n",
    "            rec_mes = TP/(TP+FN)      \n",
    "            \n",
    "    with tf.name_scope('f1_func'):\n",
    "            f1_mes = 2 * prec_mes * rec_mes / (prec_mes + rec_mes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init TensorBoard writers, Model saver, Model variables and Tfrecords coordinators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter(os.path.join(SUMMARY_DIR, 'train'),\n",
    "                                     sess.graph,\n",
    "                                     max_queue=1000)\n",
    "validation_writer = tf.summary.FileWriter(os.path.join(SUMMARY_DIR, 'validation'))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#initialization of model's variables\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "\n",
    "# Create a coordinator and run all QueueRunner objects for parallel loading of tfrecords\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definning train, validation and test functions\n",
    "\n",
    "#### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Called by trainning functiontion to validate model, it gets accuracy and loss\n",
    "of model in validation set (100k samples), save best model so far and verify validation criteria. \n",
    "If criteria is met, another PATIENCE_INCREMENT iterations of trainning will be performed.\n",
    "\n",
    "@param current_iteration\n",
    "@param current_patience trainning iterations left \n",
    "@param best_model accuracy and loss of best model so far\n",
    "@param stopping_criteria_model loss and accuracy of last model that met validation criteria\n",
    "@return current_patience or new_patience (current_iterations+PATIENCE_INCREMENT) if validation criteria\n",
    "is met \n",
    "\"\"\"\n",
    "def validate(current_iteration, current_patience, best_model, stopping_criteria_model):\n",
    "    #check if current_iteration is multiple of VALIDATION_PERIOD\n",
    "    #to perform validation every VALIDATION_PERIOD iterations\n",
    "    if current_iteration % VALIDATION_PERIOD != 0:\n",
    "        return current_patience\n",
    "    \n",
    "    #to store validation accuracy and loss\n",
    "    metric_data = {}\n",
    "    for metric in metrics:\n",
    "        metric_data[metric] = {\n",
    "            'values_per_batch': [],\n",
    "            'batch_mean': None\n",
    "        }\n",
    "\n",
    "    #Validate validation set.\n",
    "    for val_batch in range(100000 // BATCH_SIZE):\n",
    "        #get validation batch from tfrecords\n",
    "        images_array, labels_array = sess.run((\n",
    "            validation_images,\n",
    "            validation_labels))\n",
    "        #get accuracy and loss\n",
    "        metrics_value = sess.run(metrics,\n",
    "                                 feed_dict={\n",
    "                                     keep_prob: 1.0,\n",
    "                                     images: images_array,\n",
    "                                     labels: labels_array\n",
    "                                 })\n",
    "        #append every batch metric to metric_data\n",
    "        for metric, value in zip(metrics, metrics_value):\n",
    "            metric_data[metric]['values_per_batch'].append(value)\n",
    "            \n",
    "    #get accuracy and loss for al validation batches and write them to validation writer\n",
    "    #as a summary\n",
    "    for metric, name in zip(metrics, metrics_names):\n",
    "        metric_data[metric]['batch_mean'] = np.array(metric_data[metric]['values_per_batch']).mean()\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=name, simple_value=float(metric_data[metric]['batch_mean']))\n",
    "        validation_writer.add_summary(summary, current_iteration)\n",
    "    \n",
    "    #mean accuracy of validation set\n",
    "    accuracy_mean = metric_data[accuracy]['batch_mean']\n",
    "    #Check if accuracy is over best model so far and overwrite model checkpoint   \n",
    "    if accuracy_mean > best_model['accuracy']:\n",
    "        best_model['accuracy'] = accuracy_mean\n",
    "        best_model['iteration'] = current_iteration\n",
    "        print(\"New best model: Accuracy %.4f @ it %d\" % (\n",
    "            best_model['accuracy'],\n",
    "            best_model['iteration']\n",
    "        ))\n",
    "        #ckpt_dir = os.path.join(SUMMARY_DIR, 'ckpt_files')\n",
    "        #if not os.path.exists(ckpt_dir):\n",
    "        #    os.makedirs(ckpt_dir)\n",
    "        saver.save(sess, SUMMARY_DIR)\n",
    "\n",
    "    #check stopping criteria, which is:\n",
    "    #if current model error is under 99% of the error of \n",
    "    #last model that met stopping criteria, validation criteria is met\n",
    "    if (1.0-accuracy_mean) < 0.99*(1.0-stopping_criteria_model['accuracy']):\n",
    "        stopping_criteria_model['accuracy'] = accuracy_mean\n",
    "        stopping_criteria_model['iteration'] = current_iteration\n",
    "        new_patience = current_iteration + PATIENCE_INCREMENT\n",
    "        if new_patience > current_patience:\n",
    "            print(\"Patience increased to %d because of model with accuracy %.4f @ it %d\" % (\n",
    "                new_patience,\n",
    "                stopping_criteria_model['accuracy'],\n",
    "                stopping_criteria_model['iteration']\n",
    "            ))\n",
    "            return new_patience\n",
    "        else:\n",
    "            return current_patience\n",
    "    else:\n",
    "        return current_patience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate model on specific set.\n",
    "\n",
    "@param set_images\n",
    "@param set_labels\n",
    "@return loss, accuracy, precision, reacall and f1_score of model\n",
    "\"\"\"\n",
    "def eval_set(set_images, set_labels):\n",
    "    #create list of metric to store per batch evaluation\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    #get metrics on every batch\n",
    "    for test_batch in range(100000 // BATCH_SIZE):\n",
    "        images_array, labels_array = sess.run((\n",
    "            set_images,\n",
    "            set_labels))\n",
    "        loss_val, acc_val, prec, rec, f1 = sess.run((cost, acc_mes, prec_mes, rec_mes, f1_mes),\n",
    "                                     feed_dict={\n",
    "                                         keep_prob: 1.0,\n",
    "                                         images: images_array,\n",
    "                                         labels: labels_array\n",
    "                                     })\n",
    "        losses.append(loss_val)\n",
    "        accuracies.append(acc_val)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "    #average all batch metric on a single one\n",
    "    loss_mean = np.array(losses).mean()\n",
    "    accuracy_mean = np.array(accuracies).mean()\n",
    "    precision_mean = np.array(precisions).mean()\n",
    "    recall_mean = np.array(recalls).mean()\n",
    "    f1_mean = np.array(f1s).mean()\n",
    "    \n",
    "\n",
    "    return loss_mean, accuracy_mean, precision_mean, recall_mean, f1_mean\n",
    "\n",
    "\"\"\"\n",
    "Evaluate model on test set (100k samples).\n",
    "\n",
    "@return loss, accuracy, precision, reacall and f1_score of model ofer test\n",
    "\"\"\"\n",
    "def test():\n",
    "    return eval_set(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate update and trainning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_learning_rate(global_step):\n",
    "    sess.run(tf.assign(learning_rate,\n",
    "                       0.04/(2.0**(global_step//ANNEALING_PERIOD))))\n",
    "    lr_value = sess.run(learning_rate)\n",
    "    print(\"Iteration %d. Learning rate: %.4f\" % (global_step, lr_value))\n",
    "\n",
    "\"\"\"\n",
    "Train model from scratch, it means to reinitialice model parameters (weights and biases), \n",
    "set current iteration to 0 (global_step), and initialize best_model and stopping_criteria_model\n",
    "to 0.5 accuracy. After trainning process restores best model and it perform an evaluation over test set.\n",
    "\n",
    "\n",
    "@return test_accuracy, test_prec, test_rec, test_f1 of best trainning model\n",
    "\"\"\"\n",
    "def train_from_scratch():\n",
    "    #initialize params\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    #init current it to 0\n",
    "    global_step = 0\n",
    "    patience = INITIAL_PATIENCE\n",
    "    #init models acc to 0.5\n",
    "    best_model = {\n",
    "        'iteration': 0,\n",
    "        'accuracy': 0.5\n",
    "    }\n",
    "    stopping_criteria_model = {\n",
    "        'iteration': 0,\n",
    "        'accuracy': 0.5\n",
    "    }\n",
    "    #train model\n",
    "    while global_step < patience:\n",
    "        #check if learning rate must be updated\n",
    "        if global_step % ANNEALING_PERIOD == 0:\n",
    "            update_learning_rate(global_step)\n",
    "        #perform a trainning iteration\n",
    "        summaries_output, _ = sess.run((merged_summaries,\n",
    "                                        train_op),\n",
    "                                       feed_dict={\n",
    "                                           keep_prob: 0.5\n",
    "                                       })\n",
    "        global_step += 1\n",
    "        #trainning writer is commented due large amount of data that it stores\n",
    "        #write accuracy and loss summaries to train writer\n",
    "        #train_writer.add_summary(summaries_output, global_step)\n",
    "        \n",
    "        #validate model after trainning iteration\n",
    "        patience = validate(global_step, patience, best_model, stopping_criteria_model)\n",
    "    \n",
    "    #restore best model so far\n",
    "    saver.restore(sess, SUMMARY_DIR)\n",
    "    #evaluate model over test set\n",
    "    test_loss, test_accuracy, test_prec, test_rec, test_f1 = test()\n",
    "    print(\"Best model @ it %d.\\nValidation accuracy %.5f, Test accuracy %.5f\" % (\n",
    "        best_model['iteration'],\n",
    "        best_model['accuracy'],\n",
    "        test_accuracy\n",
    "    ))\n",
    "    print(\"Test loss %.5f\" % test_loss)\n",
    "    \n",
    "    return test_accuracy, test_prec, test_rec, test_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "Here you can modify variable N_models to train as many models as you want. The trainning will be sequential, and at the beginning of a new trainning, parameters of last model will be erased and reinitializated. Test and Validation metrics of every trained model are stored in lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: 0\n",
      "Iteration 0. Learning rate: 0.0400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f2cefa2bb220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nModel:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_prec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_from_scratch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0macc_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-56bbdc1104fd>\u001b[0m in \u001b[0;36mtrain_from_scratch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                                         train_op),\n\u001b[1;32m     39\u001b[0m                                        feed_dict={\n\u001b[0;32m---> 40\u001b[0;31m                                            \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                                        })\n\u001b[1;32m     42\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Numbers of model to train\n",
    "N_models = 1\n",
    "\n",
    "#list of test set parameters to save of each model\n",
    "acc_ls = []\n",
    "prec_ls = []\n",
    "rec_ls = []\n",
    "f1_ls = []\n",
    "\n",
    "#list of validation set parameters to save of each model\n",
    "acc_ls_val = []\n",
    "prec_ls_val = []\n",
    "rec_ls_val = []\n",
    "f1_ls_val = []\n",
    "\n",
    "\n",
    "for i in range(N_models):\n",
    "        \n",
    "    print(\"\\nModel:\", i)\n",
    "    test_accuracy, test_prec, test_rec, test_f1 = train_from_scratch()\n",
    "        \n",
    "    acc_ls.append(test_accuracy)\n",
    "    prec_ls.append(test_prec)\n",
    "    rec_ls.append(test_rec)\n",
    "    f1_ls.append(test_f1)\n",
    "    \n",
    "    _, val_accuracy, val_prec, val_rec, val_f1 = eval_set(validation_images, validation_labels)\n",
    "    \n",
    "    acc_ls_val.append(val_accuracy)\n",
    "    prec_ls_val.append(val_prec)\n",
    "    rec_ls_val.append(val_rec)\n",
    "    f1_ls_val.append(val_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Validation and Test Metrics means and stds of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get mean and std of a list of models parameters\n",
    "\n",
    "@metric_ls list of metrics\n",
    "@metric_name\n",
    "@return metric_mean\n",
    "@return metric_std\n",
    "\"\"\"\n",
    "def getMetricVal(metric_ls, metric_name):\n",
    "    metric_mean = np.array(metric_ls).mean()*100\n",
    "    metric_std = np.array(metric_ls).std()*100\n",
    "    print(\"%s %.2f +/- %.2f\" % (\n",
    "        metric_name,\n",
    "        np.array(metric_ls).mean()*100,\n",
    "        np.array(metric_ls).std()*100\n",
    "    ))\n",
    "    return metric_mean, metric_std\n",
    "\n",
    "print(\"Test Metrics\\n\")\n",
    "_,_=getMetricVal(acc_ls, 'Accuracy')\n",
    "\n",
    "_,_=getMetricVal(prec_ls, 'Precision')\n",
    "\n",
    "_,_=getMetricVal(rec_ls, 'Recall')\n",
    "\n",
    "_,_=getMetricVal(f1_ls, 'F1 Score')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Val Metrics\\n\")\n",
    "_,_=getMetricVal(acc_ls_val, 'Accuracy')\n",
    "\n",
    "_,_=getMetricVal(prec_ls_val, 'Precision')\n",
    "\n",
    "_,_=getMetricVal(rec_ls_val, 'Recall')\n",
    "\n",
    "_,_=getMetricVal(f1_ls_val, 'F1 Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and save Models weights and biases\n",
    "\n",
    "This will save latest trained model params, each of them will be saved as a '\\*.npy' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get parameters\n",
    "W,B = net.getWeights()\n",
    "weights, biases = sess.run([W,B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save parameters on params_path\n",
    "params_path = '/home/esteban/Esteban/LRPbeta/Code_vFloat32_TB/weights/deephits/'\n",
    "\n",
    "\"\"\"\n",
    "Save weights to *.npy files, where format is 'typeOfLayer_layerNumber_typeOfParameter.npy', for example\n",
    "for the first fully-connected layer weights will have 'FC1-W.npy'.\n",
    "\n",
    "@Ncnn number of convolutional layers with params\n",
    "@Nfc number of dense layers with params\n",
    "@weights weights from model\n",
    "@biases biases from model\n",
    "\"\"\"\n",
    "def saveWeights(Ncnn, Nfc, weights, biases):\n",
    "    for i in range(Ncnn):\n",
    "        np.save(params_path+'CNN'+str(i+1)+'-W.npy', np.array(weights[i]))\n",
    "        np.save(params_path+'CNN'+str(i+1)+'-B.npy', np.array(biases[i]))\n",
    "\n",
    "    for i in range(Nfc):\n",
    "        np.save(params_path+'FC'+str(i+1)+'-W.npy', np.array(weights[i+Ncnn]))\n",
    "        np.save(params_path+'FC'+str(i+1)+'-B.npy', np.array(biases[i+Ncnn]))\n",
    "\n",
    "#save model params\n",
    "saveWeights(5,3,weights,biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get softmax outputs to build DET curve\n",
    "\n",
    "This consumes alot of memory, we don't know why yet, but we belive that there is a memory leak in the framework.\n",
    "\n",
    "Don't run this cell if not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getParams4Metrics( set_images, set_labels, set_snr):\n",
    "    \n",
    "        lbls = []\n",
    "        probs = []\n",
    "        snr = []\n",
    "        for test_batch in range(100000 // BATCH_SIZE):\n",
    "            images_array, labels_array, snr_array = sess.run((\n",
    "                set_images,\n",
    "                set_labels,\n",
    "                set_snr))\n",
    "            probs_val = sess.run((y[:,1]), feed_dict={\n",
    "                                             keep_prob: 1.0,\n",
    "                                             images: images_array,\n",
    "                                             labels: labels_array,\n",
    "                                             phase_train: False\n",
    "                                         })\n",
    "            lbls.append(labels_array)\n",
    "            probs.append(probs_val)\n",
    "            snr.append(snr_array)\n",
    "            print(test_batch)\n",
    "\n",
    "        return np.concatenate(lbls).astype(int), np.concatenate(probs), np.concatenate(snr)\n",
    "    \n",
    "lbls, probs, snrs = getParams4Metrics(validation_images, validation_labels, validation_snr) \n",
    "\n",
    "\n",
    "np.save('/home/ayudante/Desktop/Esteban/DETparams/DHExact/val.npy', np.stack((lbls, probs, snrs[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lbls, probs, snrs = getParams4Metrics(test_images, test_labels, test_snr)\n",
    "\n",
    "np.save('/home/ayudante/Desktop/Esteban/DETparams/DHExact/test.npy', np.stack((lbls, probs, snrs[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO\n",
    "\n",
    "# - Liberate HiTS 2013 tfrecords\n",
    "# __TODO__ cite paper\n",
    "# score = tf.squeeze(score)  #TODO delete\n",
    "# __TODO__ reference paper our\n",
    "# TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
