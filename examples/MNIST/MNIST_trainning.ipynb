{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train over MNIST\n",
    "\n",
    "### Trainning model with max pooling under LRP framework to save model's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 2 and 3 comptibility\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "#enables acces to parent folder\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "#other imports\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#MNIST import\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#modules import\n",
    "import modules.sequential32 as sequential32\n",
    "import modules.linear32 as linear32\n",
    "import modules.convolution32 as convolution32\n",
    "import modules.maxpool32 as maxpool32\n",
    "import modules.avgpool32 as avgpool32\n",
    "\n",
    "\n",
    "\n",
    "#print of plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainning iterations\n",
    "max_steps = 1101 \n",
    "batch_size = 50\n",
    "learning_rate = 1e-4\n",
    "dropout = 0.5\n",
    "#where to save trainning checkpoints and tensorboard utils\n",
    "summaries_dir=\"TB/mnist/\"\n",
    "#where to save parameters of the model\n",
    "path_weights = 'weights/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute if MNIST haven't been downladed\n",
    "MNIST dataset weights approx. 12 MB. If it isn't previously downloaded on directory 'data/MNIST', it will be automatically downladed with next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively you can give a path to previously downloaded MNIST\n",
    "[only execute if previous cell wasn't]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/tesla/Desktop/MNIST_data/data/train-images-idx3-ubyte.gz\n",
      "Extracting /home/tesla/Desktop/MNIST_data/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/tesla/Desktop/MNIST_data/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/tesla/Desktop/MNIST_data/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#MNIST path\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data_dir = '/home/tesla/Desktop/MNIST_data/data'\n",
    "data = input_data.read_data_sets(data_dir, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print set lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sample in each set of MNIST:\n",
      "Training-set:\t\t55000\n",
      "Test-set:\t\t10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sample in each set of MNIST:\")\n",
    "print(\"Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"Test-set:\\t\\t{}\".format(len(data.test.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiatite network model\n",
    "\n",
    "This consider a pretreined model. To be loaded, weights must be in a 'weights_MNIST_MaxPool/' folder, relative to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn():\n",
    "    return sequential32.Sequential([convolution32.Convolution(kernel_size=5, output_depth=32, input_depth=1,\n",
    "                                   input_dim=28, act ='relu',\n",
    "                                   stride_size=1, pad='SAME'),\n",
    "                       maxpool32.MaxPool(),\n",
    "\n",
    "                       convolution32.Convolution(kernel_size=5,output_depth=64, stride_size=1,\n",
    "                                   act ='relu', pad='SAME'),\n",
    "                       maxpool32.MaxPool(),\n",
    "                       \n",
    "                       linear32.Linear(1024, act ='relu'),\n",
    "\n",
    "                       linear32.Linear(10, act ='linear')])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass ... \n",
      "------------------------------------------------- \n",
      "input:: [None, 28, 28, 1]\n",
      "conv2d_1:: [None, 28, 28, 32]\n",
      "maxpool_2:: [None, 14, 14, 32]\n",
      "conv2d_3:: [None, 14, 14, 64]\n",
      "maxpool_4:: [None, 7, 7, 64]\n",
      "linear_5:: [None, 1024]\n",
      "linear_6:: [None, 10]\n",
      "softmax:: [None, 10]\n",
      "\n",
      "------------------------------------------------- \n"
     ]
    }
   ],
   "source": [
    "#Dropout placeholder\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep-prob')\n",
    "\n",
    "#Input place holders\n",
    "with tf.name_scope('input'):\n",
    "        #stetched image\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "        #target label\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
    "  \n",
    "        #reshape input as image tensor with an undifined batch size (-1)\n",
    "        inp = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "#Model instance\n",
    "with tf.variable_scope('model'):\n",
    "        #instanciate model\n",
    "        net = nn()\n",
    "\n",
    "        #feed-forward method and get score output. This iterates through layer in net object of class Sequiential\n",
    "        score = net.forward(inp)\n",
    "\n",
    "        #pass scores through softmax for network output ([0,1] probability)   \n",
    "        y = tf.nn.softmax(score)\n",
    "\n",
    "        #predicted classes\n",
    "        y_pred_cls = tf.argmax(y, 1)\n",
    "\n",
    "        #true classes\n",
    "        y_true_cls = tf.argmax(y_, 1)\n",
    "        \n",
    "        ##Training function      \n",
    "        trainer = net.fit(output=score, ground_truth=y_, opt_params=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement and create TensorBoard writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):  \n",
    "        #performance measures\n",
    "\n",
    "        #compare predictions\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "        #get accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge([accuracy_sum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Tensorflow session, variables and tensorboard writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "train_writer = tf.summary.FileWriter(summaries_dir + '/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter(summaries_dir + '/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to feed model with data\n",
    "Generates proper functions to construct dictionary that will be feed to model graph. Define batch, labels and dropout.\n",
    "\n",
    "There is the option to feed the model with a batch of MNIST, or feed it with a ten digits example.\n",
    "\n",
    "__Remember:__ Images pixels are 0 centered in values [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get MNIST Data to feed model. You can choose to feed a train batch or the whole test set \n",
    "def feed_dict(mnist, train, batch_s=None):\n",
    "    if train:\n",
    "        xs, ys = mnist.train.next_batch(batch_s)\n",
    "        k = dropout\n",
    "    else:\n",
    "        xs, ys = mnist.test.images, mnist.test.labels\n",
    "        k = 1.0\n",
    "    return (2*xs)-1, ys, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Check accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Measure accuracy of the net\n",
    "def print_test_accuracy(test_batch_size, mnist):\n",
    "    data=mnist \n",
    "    data.test.cls = np.argmax(data.test.labels, axis=1)\n",
    "    #data.validation.cls = np.argmax(data.test.labels, axis=1)\n",
    "    #data.train.cls = np.argmax(data.train.labels, axis=1)\n",
    "    # Numero de imagenes en test-set.\n",
    "    num_test = len(data.test.images)\n",
    "\n",
    "    # Crea arreglo para guardar clases predichas.\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "\n",
    "    # Calcular clases predichas.\n",
    "    i = 0\n",
    "    while i < num_test:\n",
    "        \n",
    "        j = min(i + test_batch_size, num_test)\n",
    "        images = data.test.images[i:j, :]\n",
    "        images = (2*images)-1\n",
    "        labels = data.test.labels[i:j, :]\n",
    "        dp=1\n",
    "        feed_dict = {x: images,\n",
    "                     y_: labels, keep_prob: dp}\n",
    "\n",
    "        cls_pred[i:j] = sess.run(y_pred_cls, feed_dict=feed_dict)\n",
    "        i = j\n",
    "    \n",
    "    # Labels reales.\n",
    "    cls_true = data.test.cls\n",
    "\n",
    "    # Arreglo booleano de clasificaciones correctas.\n",
    "    correct = (cls_true == cls_pred)\n",
    "    \n",
    "    #Numero de clasificaciones correctas.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Accuracy\n",
    "    acc = float(correct_sum) / num_test\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "    return acc, cls_true,  cls_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "train model for max_steps iterations and print accuracy at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoca:      1\n",
      "Accuracy on Test-Set: 16.6% (1657.00003505 / 10000)\n",
      "Iterations:      0, Training Accuracy:  20.0%\n",
      "Iterations:    110, Training Accuracy:  92.0%\n",
      "Iterations:    220, Training Accuracy:  90.0%\n",
      "Iterations:    330, Training Accuracy:  90.0%\n",
      "Iterations:    440, Training Accuracy:  88.0%\n",
      "Iterations:    550, Training Accuracy:  90.0%\n",
      "Iterations:    660, Training Accuracy: 100.0%\n",
      "Iterations:    770, Training Accuracy: 100.0%\n",
      "Iterations:    880, Training Accuracy:  96.0%\n",
      "Iterations:    990, Training Accuracy:  90.0%\n",
      "epoca:      2\n",
      "Accuracy on Test-Set: 97.6% (9764.99974728 / 10000)\n",
      "Iterations:   1100, Training Accuracy:  98.0%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fd3a7210f910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#Train_acc[int(i/(ep/10)),:]=[i,acc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "#TRAIN\n",
    "j=0\n",
    "ep=55000/batch_size\n",
    "\n",
    "\n",
    "for i in range(max_steps):\n",
    "    d = feed_dict(data, True, batch_size)\n",
    "    inp = {x:d[0], y_:d[1], keep_prob:d[2]}\n",
    "    #inp = feed_dict(mnist, True, batch_size)\n",
    "    \n",
    "    sess.run(trainer.train, feed_dict=inp)\n",
    "\n",
    "    if i % ep == 0:\n",
    "        d_t = feed_dict(data, False, batch_size)\n",
    "        inp_t = {x:d_t[0], y_:d_t[1], keep_prob:d_t[2]}\n",
    "        summary, acc = sess.run([merged, accuracy], feed_dict=inp_t)\n",
    "        test_writer.add_summary(summary, i)\n",
    "        msg = \"epoca: {0:>6}\"\n",
    "        print(msg.format(j+1))\n",
    "        #print_test_accuracy(test_batch_size=batch_size, mnist=mnist)\n",
    "        num_test = len(data.test.images)\n",
    "        correct_sum = acc*num_test\n",
    "        msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "        print(msg.format(acc, correct_sum, num_test))\n",
    "        j+=1\n",
    "        \n",
    "        \n",
    "    if i % (ep/10) == 0:\n",
    "        inp = {x:d[0], y_:d[1], keep_prob:d_t[2]}\n",
    "        summary, acc = sess.run([merged, accuracy], feed_dict=inp) #accuracy on batch\n",
    "        train_writer.add_summary(summary, i)\n",
    "        msg = \"Iterations: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "        print(msg.format(i, acc))\n",
    "        #Train_acc[int(i/(ep/10)),:]=[i,acc]\n",
    "\n",
    "print_test_accuracy(test_batch_size=batch_size, mnist=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and save Models weights and biases\n",
    "\n",
    "This will save latest trained model params, each of them will be saved as a '\\*.npy' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer without params\n",
      "layer without params\n"
     ]
    }
   ],
   "source": [
    "#get parameters\n",
    "W,B = net.getWeights()\n",
    "weights, biases = sess.run([W,B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save parameters on params_path\n",
    "params_path = path_weights\n",
    "\n",
    "\"\"\"\n",
    "Save weights to *.npy files, where format is 'typeOfLayer_layerNumber_typeOfParameter.npy', for example\n",
    "for the first fully-connected layer weights will have 'FC1-W.npy'.\n",
    "\n",
    "@Ncnn number of convolutional layers with params\n",
    "@Nfc number of dense layers with params\n",
    "@weights weights from model\n",
    "@biases biases from model\n",
    "\"\"\"\n",
    "def saveWeights(Ncnn, Nfc, weights, biases):\n",
    "    for i in range(Ncnn):\n",
    "        np.save(params_path+'CNN'+str(i+1)+'-W.npy', np.array(weights[i]))\n",
    "        np.save(params_path+'CNN'+str(i+1)+'-B.npy', np.array(biases[i]))\n",
    "\n",
    "    for i in range(Nfc):\n",
    "        np.save(params_path+'FC'+str(i+1)+'-W.npy', np.array(weights[i+Ncnn]))\n",
    "        np.save(params_path+'FC'+str(i+1)+'-B.npy', np.array(biases[i+Ncnn]))\n",
    "\n",
    "#save model params\n",
    "saveWeights(2,2,weights,biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
